---
title: Information Theory
---

# Information Theory

## Common Metrics

Entropy

Pointwise Mutual Information

$$
\text{pmi} (x;y)\equiv \log {\frac {p(x,y)}{p(x)p(y)}}=\log {\frac {p(x|y)}{p(x)}}=\log {\frac {p(y|x)}{p(y)}}
$$
